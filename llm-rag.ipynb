{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f45113e2-9684-4b07-87c2-acfec080b31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /opt/conda/envs/pytorch/lib/python3.10/site-packages (2.3.1)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-2.4.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sentence-transformers) (4.37.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sentence-transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sentence-transformers) (1.26.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sentence-transformers) (1.4.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sentence-transformers) (1.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: Pillow in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sentence-transformers) (10.2.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.12.2)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.15.1->sentence-transformers) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Downloading sentence_transformers-2.4.0-py3-none-any.whl (149 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.5/149.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentence-transformers\n",
      "  Attempting uninstall: sentence-transformers\n",
      "    Found existing installation: sentence-transformers 2.3.1\n",
      "    Uninstalling sentence-transformers-2.3.1:\n",
      "      Successfully uninstalled sentence-transformers-2.3.1\n",
      "Successfully installed sentence-transformers-2.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f71fd0f0-eeaf-40e1-85d0-cebaa32e4747",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "#e5-mistral-7b-instruct,paraphrase-MiniLM-L6-v2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Sentences we want to encode. Example:\n",
    "sentence = ['This framework generates embeddings for each input sentence']\n",
    "\n",
    "# Sentences are encoded by calling model.encode()\n",
    "embedding = model.encode(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4e5dbde-cc5f-49d8-ad10-175146554d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f89ca2d0-3f65-42b9-b483-e5670e81f505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faec073b-bba6-4107-b203-5c641bf09267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.12.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from openai) (4.2.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.26.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from openai) (2.6.0)\n",
      "Requirement already satisfied: sniffio in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.16.1)\n",
      "Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.3-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: h11, distro, httpcore, httpx, openai\n",
      "Successfully installed distro-1.9.0 h11-0.14.0 httpcore-1.0.3 httpx-0.26.0 openai-1.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03940500-8ff5-4c68-906e-2287e3a54877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='ae90f5db-bf93-43c3-82a4-c0a999579d3d', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The 2020 World Series was played at Arlington, Texas, at the Globe Life Field and the American Airlines Center.\\n', role='assistant', function_call=None, tool_calls=None))], created=1710133036, model='mistral', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=0, prompt_tokens=0, total_tokens=0))\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://127.0.0.1:8080/v1\", api_key=\"NONE\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "        model=\"mistral\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "        ]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21d018a4-4fe7-4d90-9674-4ad6220d3604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='7b263a9a-74cf-4ae2-817c-7407675e7c15', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The 2020 World Series was played at Arlington, Texas, at the Globe Life Field and the American Airlines Center.\\n', role='assistant', function_call=None, tool_calls=None))], created=1709095644, model='mistral', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=0, prompt_tokens=0, total_tokens=0))\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:8080/v1\", api_key=\"NONE\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "        model=\"mistral\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "        ]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86276239-57dc-4ef5-bdb2-64811dede767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://127.0.0.1:8080/v1\", api_key=\"NONE\")\n",
    "\n",
    "response = client.embeddings.create(\n",
    "        input=\"Your text string goes here\",\n",
    "        model=\"text-embedding-ada-002\"\n",
    ")\n",
    "\n",
    "print(len(response.data[0].embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3efdb46b-890f-4eb4-904f-402fce355792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting rich\n",
      "  Downloading rich-13.7.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from rich) (2.17.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.7.0-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.6/240.6 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdf2, mdurl, markdown-it-py, rich\n",
      "Successfully installed markdown-it-py-3.0.0 mdurl-0.1.2 pypdf2-3.0.1 rich-13.7.0\n"
     ]
    }
   ],
   "source": [
    "! pip install pypdf2 rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2d51a8c-03d2-4e02-9d8c-38d28b756ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import PyPDF2\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rich.console import Console\n",
    "from rich.progress import track, Progress\n",
    "from pathlib import Path\n",
    "from functools import cache\n",
    "\n",
    "from prompts import DEFAULT_QUERY_PROMPT_TMPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57c2b33d-f7b0-46a8-9790-9fe7b0e2f4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "console = Console()\n",
    "client = OpenAI(base_url=\"http://localhost:8080/v1\", api_key=\"NONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e16401c1-0a1d-4ae3-bd53-be171f234cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"./cache\"\n",
    "PDF_URL = \"https://arxiv.org/pdf/1706.03762.pdf\"\n",
    "Path(CACHE_DIR).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03798f4e-244b-4a76-9d6f-e0f9bd784f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cache\n",
    "def get_embedding(text):\n",
    "    response = client.embeddings.create(\n",
    "\t\tinput=text,\n",
    "\t\tmodel=\"text-embedding-ada-002\"\n",
    "\t)\n",
    "    \n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f772eb2-0e6a-4927-9899-b32560d897c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_model(prompt):\n",
    "\tresponse = client.chat.completions.create(\n",
    "\t\tmodel=\"mistral\",\n",
    "\t\tmessages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "\t\ttemperature=0.7,\n",
    "\t\tmax_tokens=512,\n",
    "\t\ttop_p=1,\n",
    "\t\tfrequency_penalty=0,\n",
    "\t\tpresence_penalty=0\n",
    "\t)\n",
    "\t\n",
    "\treturn response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e389830-11e0-48dc-85c4-20cc844cc390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b543094-ae91-4913-80b3-edaba763ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pdf(pdf_url, query):\n",
    "    pdf_filename = Path(pdf_url).name\n",
    "    \n",
    "    local_file = Path(CACHE_DIR) / pdf_filename\n",
    "    \n",
    "    # first get the pdf if not there\n",
    "    if not local_file.exists():\n",
    "        with console.status(\"[bold yellow]downloading pdf...\", spinner=\"runner\"):\n",
    "            # download and save in cache dir\n",
    "            pdf_res = requests.get(pdf_url, timeout=300)\n",
    "            # cache the pdf\n",
    "            local_file.write_bytes(pdf_res.content)\n",
    "\n",
    "        console.log(\"[bold green]pdf downloaded!\")\n",
    "    \n",
    "    # check if embeddings exists in cache\n",
    "    embedding_file = local_file.parent / f\"{local_file.stem}.embeddings.json\"\n",
    "    \n",
    "    if embedding_file.exists():\n",
    "        with console.status(f\"[bold yellow]loading embedding index from cache...\", spinner=\"balloon\"):\n",
    "            with embedding_file.open(\"r\") as f:\n",
    "                embeddings_index = json.load(f)\n",
    "            console.log(\"[bold green]embedding index loaded from cache\")\n",
    "    else:\n",
    "        with console.status(f\"[bold yellow]computing embeddings and index...\", spinner=\"earth\"):\n",
    "            with local_file.open(\"rb\") as f:\n",
    "                pdf = PyPDF2.PdfReader(f)\n",
    "                # compute embeddings and save to disk\n",
    "                num_pages = len(pdf.pages)\n",
    "                console.log(f\"got pages: {num_pages=}\")\n",
    "            \n",
    "                embeddings_index = {\n",
    "                    \"sequence\": [],\n",
    "                    \"docs_store\": {},\n",
    "                    \"vecs_store\": {}\n",
    "                }\n",
    "                \n",
    "                for page_num in track(range(num_pages), description=\"querying embeddings...\"):\n",
    "                    page_uuid = str(uuid.uuid4())\n",
    "                    pdf_text = pdf.pages[page_num].extract_text()\n",
    "                    \n",
    "                    embeddings_index['docs_store'][page_uuid] = {\n",
    "                        \"id\": page_uuid,\n",
    "                        \"text\": pdf_text,\n",
    "                        \"page_num\": page_num\n",
    "                    }\n",
    "                    \n",
    "                    text_embedding = get_embedding(pdf_text)\n",
    "\n",
    "                    embeddings_index['vecs_store'][page_uuid] = {\n",
    "                        \"id\": page_uuid,\n",
    "                        \"vector\": text_embedding,\n",
    "                    }\n",
    "                    \n",
    "                    embeddings_index['sequence'].append(page_uuid)\n",
    "                    \n",
    "                # save this index\n",
    "                with embedding_file.open(\"w\") as f:\n",
    "                    json.dump(embeddings_index, f)\n",
    "\n",
    "        console.log(\"[bold green]done computing embedding index\")\n",
    "        \n",
    "    # create embeddings numpy\n",
    "    np_index_to_uuid = []\n",
    "    embeddings_np = []\n",
    "    for emb_uuid, emb in embeddings_index['vecs_store'].items():\n",
    "        np_index_to_uuid.append(emb_uuid)\n",
    "        embeddings_np.append(emb['vector'])\n",
    "    embeddings_np = np.array(embeddings_np)\n",
    "    \n",
    "    # get query embed\n",
    "    query_emb = get_embedding(query)\n",
    "    \n",
    "    # find the closest text to the query in doc store\n",
    "    with console.status(f\"[bold yellow]finding relevant text in doc...\", spinner=\"earth\"):\n",
    "        relevant_text = embeddings_index['docs_store'][\n",
    "            np_index_to_uuid[\n",
    "                np.argmax([\n",
    "                    cosine_similarity(query_emb, emb) for emb in embeddings_np\n",
    "                ])\n",
    "            ]\n",
    "        ]['text']\n",
    "    console.log(\"[bold green]found relevant text from input\")\n",
    "    \n",
    "    query_text = DEFAULT_QUERY_PROMPT_TMPL.substitute(\n",
    "        context_str=relevant_text,\n",
    "        query_str=query\n",
    "    )\n",
    "    \n",
    "    with console.status(f\"[bold yellow]quering the model...\"):\n",
    "        model_out = query_model(prompt=query_text)\n",
    "    \n",
    "    console.log(f\"[bold yellow]Question: {query}\")\n",
    "    console.log(f\"[bold yellow]Context: {local_file.name}\")\n",
    "    console.log(f\"[bold green]Answer: {model_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65d55f56-c482-4912-b4de-27d9ed47dc1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05:38:49] </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">embedding index loaded from cache</span>                                                         <a href=\"file:///tmp/ipykernel_1705/41403016.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">41403016.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_1705/41403016.py#23\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">23</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05:38:49]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;32membedding index loaded from cache\u001b[0m                                                         \u001b]8;id=364261;file:///tmp/ipykernel_1705/41403016.py\u001b\\\u001b[2m41403016.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=200628;file:///tmp/ipykernel_1705/41403016.py#23\u001b\\\u001b[2m23\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">found relevant text from input</span>                                                            <a href=\"file:///tmp/ipykernel_1705/41403016.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">41403016.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_1705/41403016.py#83\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">83</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;32mfound relevant text from input\u001b[0m                                                            \u001b]8;id=286012;file:///tmp/ipykernel_1705/41403016.py\u001b\\\u001b[2m41403016.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=342464;file:///tmp/ipykernel_1705/41403016.py#83\u001b\\\u001b[2m83\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05:40:17] </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Question: explain the attention mechanism</span>                                                 <a href=\"file:///tmp/ipykernel_1705/41403016.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">41403016.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_1705/41403016.py#93\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">93</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05:40:17]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;33mQuestion: explain the attention mechanism\u001b[0m                                                 \u001b]8;id=848656;file:///tmp/ipykernel_1705/41403016.py\u001b\\\u001b[2m41403016.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=608898;file:///tmp/ipykernel_1705/41403016.py#93\u001b\\\u001b[2m93\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Context: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">1706.03762</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">.pdf</span>                                                                   <a href=\"file:///tmp/ipykernel_1705/41403016.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">41403016.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_1705/41403016.py#94\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">94</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;33mContext: \u001b[0m\u001b[1;33m1706.03762\u001b[0m\u001b[1;33m.pdf\u001b[0m                                                                   \u001b]8;id=10096;file:///tmp/ipykernel_1705/41403016.py\u001b\\\u001b[2m41403016.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=109292;file:///tmp/ipykernel_1705/41403016.py#94\u001b\\\u001b[2m94\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Answer: The attention mechanism is a technique used in deep learning models, particularly</span> <a href=\"file:///tmp/ipykernel_1705/41403016.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">41403016.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_1705/41403016.py#95\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">95</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">in sequence transduction models like neural machine translation, to selectively focus on </span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">certain parts of the input sequence. It is a weighting function that determines the </span>      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">importance of each input element in the output.</span>                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In the context of the Transformer model mentioned in the paper, the attention mechanism </span>  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">is a key component of the model's architecture. It is based on the idea of </span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">self-attention, which allows the model to learn relationships between different parts of </span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">the input sequence without the need for complex recurrent or convolutional neural </span>        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">networks.</span>                                                                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">The attention mechanism works by first representing the input sequence as a matrix of </span>    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">query (Q), key (K), and value (V) vectors. These vectors are typically learned during the</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">training process. Then, a weighted sum of the value vectors is calculated based on the </span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">importance of each input element, as determined by the attention mechanism.</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">The attention mechanism is calculated as a scaled dot-product of the Q and K vectors, </span>    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">followed by a non-linear activation function. The scaling factor helps to prevent the </span>    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">attention mechanism from becoming unstable due to the large number of input elements.</span>     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In the Transformer model, the attention mechanism is used in three different ways:</span>        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">. Self-attention: The model learns to attend to different parts of the input sequence to</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">determine the importance of each element in the output.</span>                                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">. Encoder-decoder attention: The model learns to attend to the output sequence (encoder)</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">based on the input sequence (decoder).</span>                                                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">. Multi-head attention: The model learns multiple attention mechanisms in parallel, each</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">focusing on different aspects of the input sequence.</span>                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">The attention mechanism allows the Transformer model to learn long-range dependencies and</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">complex relationships between input elements, leading to improved performance in tasks </span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">such as machine translation.&lt;dummy32000&gt;</span>                                                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;32mAnswer: The attention mechanism is a technique used in deep learning models, particularly\u001b[0m \u001b]8;id=875547;file:///tmp/ipykernel_1705/41403016.py\u001b\\\u001b[2m41403016.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=216781;file:///tmp/ipykernel_1705/41403016.py#95\u001b\\\u001b[2m95\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;32min sequence transduction models like neural machine translation, to selectively focus on \u001b[0m \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;32mcertain parts of the input sequence. It is a weighting function that determines the \u001b[0m      \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;32mimportance of each input element in the output.\u001b[0m                                           \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                                                                                          \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;32mIn the context of the Transformer model mentioned in the paper, the attention mechanism \u001b[0m  \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;32mis a key component of the model's architecture. It is based on the idea of \u001b[0m               \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;32mself-attention, which allows the model to learn relationships between different parts of \u001b[0m \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;32mthe input sequence without the need for complex recurrent or convolutional neural \u001b[0m        \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;32mnetworks.\u001b[0m                                                                                 \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                                                                                          \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;32mThe attention mechanism works by first representing the input sequence as a matrix of \u001b[0m    \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;32mquery \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mQ\u001b[0m\u001b[1;32m)\u001b[0m\u001b[1;32m, key \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mK\u001b[0m\u001b[1;32m)\u001b[0m\u001b[1;32m, and value \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mV\u001b[0m\u001b[1;32m)\u001b[0m\u001b[1;32m vectors. These vectors are typically learned during the\u001b[0m \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;32mtraining process. Then, a weighted sum of the value vectors is calculated based on the \u001b[0m   \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;32mimportance of each input element, as determined by the attention mechanism.\u001b[0m               \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                                                                                          \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;32mThe attention mechanism is calculated as a scaled dot-product of the Q and K vectors, \u001b[0m    \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;32mfollowed by a non-linear activation function. The scaling factor helps to prevent the \u001b[0m    \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;32mattention mechanism from becoming unstable due to the large number of input elements.\u001b[0m     \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                                                                                          \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;32mIn the Transformer model, the attention mechanism is used in three different ways:\u001b[0m        \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                                                                                          \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;32m1\u001b[0m\u001b[1;32m. Self-attention: The model learns to attend to different parts of the input sequence to\u001b[0m \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;32mdetermine the importance of each element in the output.\u001b[0m                                   \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;32m2\u001b[0m\u001b[1;32m. Encoder-decoder attention: The model learns to attend to the output sequence \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mencoder\u001b[0m\u001b[1;32m)\u001b[0m \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;32mbased on the input sequence \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mdecoder\u001b[0m\u001b[1;32m)\u001b[0m\u001b[1;32m.\u001b[0m                                                    \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;32m3\u001b[0m\u001b[1;32m. Multi-head attention: The model learns multiple attention mechanisms in parallel, each\u001b[0m \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;32mfocusing on different aspects of the input sequence.\u001b[0m                                      \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                                                                                          \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;32mThe attention mechanism allows the Transformer model to learn long-range dependencies and\u001b[0m \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;32mcomplex relationships between input elements, leading to improved performance in tasks \u001b[0m   \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;32msuch as machine translation.\u001b[0m\u001b[1;32m<\u001b[0m\u001b[1;32mdummy32000\u001b[0m\u001b[1;32m>\u001b[0m                                                  \u001b[2m              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_pdf(\n",
    "\tpdf_url=PDF_URL,\n",
    "\tquery=\"explain the attention mechanism\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b41a03bc-1966-4493-8489-ac640ddf3f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eefc02a7-8d4e-4522-b9c5-2b86f0336cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ee\n",
      "\n",
      "   \n",
      "\n",
      "PLOT ayill\n",
      "\n",
      "yl bbao\n",
      "Land Site Plan\n",
      "\n",
      "Gb i tlg GLa Lull gil\n",
      "DEPARTMENT OF MUNICIPALITIES\n",
      "AND TRANSPORT\n",
      "\n",
      "3\n",
      "\n",
      "  \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "  \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Notes and Conditions:\n",
      "\n",
      "» This plan is to clarify the boundaries and parameters of the property only. It\n",
      "shall not be considered asa ttle deed, nor be used in place of it. The mentioned\n",
      "iamensions and area are considered preliminary, and the actual area will be fixed\n",
      "after surveying and according to nature\n",
      "\n",
      "* Any alteration or modification of, or addition to the document\n",
      "rendersit void\n",
      "\n",
      "* This contract has been issued electronically, and its validity can be\n",
      "verified through the link: https://smarthub. adm.gov.ae\n",
      "2023/627086\n",
      "\n",
      "Application Number. ‘alolaall aay\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "see Une py Y&Kla as py ¥ dash Jl lang gn epangl ala fa *\n",
      "Anglo pole isu lal in ghj yall AO yy\n",
      "\n",
      "i ja crac a hig ect users\n",
      "\n",
      "-ulyll Jus 40 aia Go gaaill j4aig uigyiSll aidusall Ihe ya *\n",
      "\n",
      " \n",
      "\n",
      "Municipality ‘Abu Dhabi City uboll auao all\n",
      "Zone Yas Island ul ys)\n",
      "Sector YN2 02 Jlauts uly\n",
      "Road Name - -\n",
      "Plot Area 510.00 m2/5,489.53 ft?\n",
      "Land No, 540\n",
      "Plot Address 135-0-000-540\n",
      "Construction status | Not Constructed aan ye\n",
      "Allocation Type Residenti piu\n",
      "Land Use investment- residentialVilla Suid alas - syladiual\n",
      "Base District (DEVELOPMENT PLANNED)anb cillbio gblio\n",
      "Overlay District CSZ,UGB,W\n",
      "Allocation Date 2019/06/10\n",
      "Project No. 2019/222253\n",
      "Project Name LEA Ww\n",
      "Developer Name | ALDAR PROPERTIES - PJ SC & 2 ult ylaall jal\n",
      "OWNER DETAILS\n",
      "ajlall 9 anal premey] pull 4\n",
      "Share and Acquisition Nationality Name\n",
      "%100 alyatillg gull uilall oaiaagiyl Juli! bail, 4\n",
      "i Buy and Sell 100% Ownership German BETTINA ISABEL PARTZSCH\n",
      "https://smarthub.adm.gov.ae\n",
      "Page 1 of Application Date: 2023/10/31 “Alaleall ul\n"
     ]
    }
   ],
   "source": [
    "# Load your image\n",
    "image_path = '/home/ubuntu/Site_Plan_New_10-2023_page-0001.jpg'\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Use pytesseract to do OCR on the image\n",
    "text = pytesseract.image_to_string(image)\n",
    "trimmed_text = text.strip()\n",
    "\n",
    "print(trimmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3853ae0b-6c91-4b76-8f5c-30de24cbd462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_model(prompt):\n",
    "\tresponse = client.chat.completions.create(\n",
    "\t\tmodel=\"mistral\",\n",
    "\t\tmessages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "\t\ttemperature=0.01,\n",
    "\t\tmax_tokens=512,\n",
    "\t\ttop_p=1,\n",
    "\t\tfrequency_penalty=0,\n",
    "\t\tpresence_penalty=0\n",
    "\t)\n",
    "\t\n",
    "\treturn response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9bb15742-0109-4573-b806-42ca4dffb94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"please return values of  plot area and land no in a json format only consider meaningful words in english present\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e75d44a1-26bd-4910-b428-2ee3f09c7727",
   "metadata": {},
   "outputs": [],
   "source": [
    "#relevant_text = \"Plot Area 510.00 m*/5,489.53 Ft? aul aun, Land No 540 Aull Plot Address 135-0-000-540 Saquiall olgie\"\n",
    "  \n",
    "query_text = DEFAULT_QUERY_PROMPT_TMPL.substitute(\n",
    "  context_str=trimmed_text,\n",
    "  query_str=question\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ac104d4-ea03-4127-9ed9-f20fd5e7d501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Context information is below.\\n\\n----------\\nee\\n\\n   \\n\\nPLOT ayill\\n\\nyl bbao\\nLand Site Plan\\n\\nGb i tlg GLa Lull gil\\nDEPARTMENT OF MUNICIPALITIES\\nAND TRANSPORT\\n\\n3\\n\\n  \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n  \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nNotes and Conditions:\\n\\n» This plan is to clarify the boundaries and parameters of the property only. It\\nshall not be considered asa ttle deed, nor be used in place of it. The mentioned\\niamensions and area are considered preliminary, and the actual area will be fixed\\nafter surveying and according to nature\\n\\n* Any alteration or modification of, or addition to the document\\nrendersit void\\n\\n* This contract has been issued electronically, and its validity can be\\nverified through the link: https://smarthub. adm.gov.ae\\n2023/627086\\n\\nApplication Number. ‘alolaall aay\\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nsee Une py Y&Kla as py ¥ dash Jl lang gn epangl ala fa *\\nAnglo pole isu lal in ghj yall AO yy\\n\\ni ja crac a hig ect users\\n\\n-ulyll Jus 40 aia Go gaaill j4aig uigyiSll aidusall Ihe ya *\\n\\n \\n\\nMunicipality ‘Abu Dhabi City uboll auao all\\nZone Yas Island ul ys)\\nSector YN2 02 Jlauts uly\\nRoad Name - -\\nPlot Area 510.00 m2/5,489.53 ft?\\nLand No, 540\\nPlot Address 135-0-000-540\\nConstruction status | Not Constructed aan ye\\nAllocation Type Residenti piu\\nLand Use investment- residentialVilla Suid alas - syladiual\\nBase District (DEVELOPMENT PLANNED)anb cillbio gblio\\nOverlay District CSZ,UGB,W\\nAllocation Date 2019/06/10\\nProject No. 2019/222253\\nProject Name LEA Ww\\nDeveloper Name | ALDAR PROPERTIES - PJ SC & 2 ult ylaall jal\\nOWNER DETAILS\\najlall 9 anal premey] pull 4\\nShare and Acquisition Nationality Name\\n%100 alyatillg gull uilall oaiaagiyl Juli! bail, 4\\ni Buy and Sell 100% Ownership German BETTINA ISABEL PARTZSCH\\nhttps://smarthub.adm.gov.ae\\nPage 1 of Application Date: 2023/10/31 “Alaleall ul\\n----------\\n\\nGiven the context information and not prior knowledge, answer the question: please return values of  plot area and land no in a json format only consider meaningful words in english present\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92db9873-5e89-4e76-80d8-e38fe478b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "117b3e88-50ee-4a70-a0af-3808afdb822e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.9 s ± 49 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit query_model(query_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63280fc4-de91-412e-81ac-a73afd98de3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
